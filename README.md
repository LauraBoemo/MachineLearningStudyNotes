# â˜• Machine Learning and AWS...

Hi! Welcome to my Machine Learning on AWS study notes! ğŸ‘‹ I strongly hope this document is as useful to you as it is to me. Have fun! ğŸ‰

<br>

# 1. Data Engineering

## 1. Where it all starts... Amazon S3

### ğŸ“Œ AWS S3 Overview
Amazon S3 allows people to **store Objects** (any type of file format, such as CSV, JSON, Parquet, ORC, Avro, Protobuf, etc; as long as it contains a maximum size of 5TB) in **Buckets** (same as directories);

âœï¸ To Remember...
- Every object has a Key. The Key is the FULL path...
    - This full path will be interesting when we look at **partitioning** (related to Kinesis and Athena);
    - Those tags can be associated with the Object Tag; which are helpful to classify data, security and lifecycle;
- Buckets have a _globally unique name_;

ğŸŒ® Examples...
```
<my_bucket>/my_file.txt
```
```
<my_bucket>/my_filder/another_folder/my_file.txt
```
<br>

### ğŸ“Œ AWS S3 for Machine Learning
S3 is the backbone for many AWS ML Services (eg.: SageMaker). It is helpful to create a **Data Lake**, a few factors for this are...

- S3 has infinite size with no provisioning (which means it is possible to create a centralized architecture - maintaining all the data in one place).
    - By the way, it does not only about maintaining but also about safety: S3 has 99.999999999% of durability.
- Decoupling of storage (S3) to compute (EC2, Amazon Athena, Amazon Redshift Spectrum, Amazon Rekognition, and AWS Glue) - the data in S3 can be used with all those compute tools, **which create a massive Data Lake in S3**.

<br>

### AWS S3 Data Partitioning
AWS S3 allows us to create a pattern for speeding up range queries (eg.: AWS Athena), called **partitioning** - there are infinite ways to define whatever partitioning strategy; which will be handled by some AWS tools, like AWS Glue.

ğŸŒ® Examples...

```
By date: 

s3://bucket/my_data_set/year/month/day/hour/data_00.csv

"/year/month/day/hour/" is the practition
```
```
By product: 

s3://bucket/my_data_set/product-id/data_01.csv

"/product-id/" is the practition
```
<br>

### ğŸ“Œ AWS S3 Storage Tiers & Lifecycle Rules

ğŸ“ There are different storage tiers on S3...
- Amazon S3 Standart - General Purpose and default **<< Important for AWS ML**
```
    - Durability -- 99.999999999%
    - Availability -- 99.99%
    - AZ -- >= 3
    - Concurrent facility fault tolerance -- 2
```
- Amazon S3 Standart - Infrequent Access (IA)
- Amazon S3 One Zone - Infrequent Access
- Amazon S3 Intelligent Tiering
- Amazon Glacier **<< Important for AWS ML**
```
    - Durability -- 99.999999999%
    - Availability -- NA
    - AZ -- >= 3
    - Concurrent facility fault tolerance -- 1
```

Is possible set lifecycle rules in S3, it moves the data between different tiers, to save storage cost.

ğŸŒ® Examples...

```
General Purpose > Infrequent Access > Glacier
```
ğŸ“ 2 ways to set lifecycle rules...
1. Transition Actions
    - Objects are transitioned to another storage class.
        - "Move objects to Standart IA class 60 days after creation and then move to Glacier for archiving after 6 months"
2. Expiration Actions
    - S3 deletes expired objects on our behalf
        - Access log files can be set to delete after a specified period of time

<br>

### ğŸ“Œ AWS S3 Encryption for Objects 

ğŸ“ There are 4 methods of encrypting objects in S3...
- SSE-S3 - Encrypts S3 objects using keys handled & managed by AWS **<< Important for the AWS ML**
- SSE-KMS - use AWS Key Management Service to manage encryption keys **<< Important for the AWS ML**
    - Additional security (user must have access to KMS key)
    - Audit trail for KMS usage
- SSE-C - when you want to manage your own encryption keys
- Client Side Encryption

But, from a ML perspective, SSE-S3 and SSE-KMS will be most likely used. So...

- SSE-S3
    - Object -- HTTP/S + Header -- | *Amazon S3 --> Object + S3 Managed Data Key -- Encryption --> Bucket | 
    
- SSE-KMS
    - Object -- HTTP/S + Header -- | *Amazon S3 --> Object + KMS Customer Master Key (CMK) -- Encryption --> Bucket | 
    
<br>

### ğŸ“Œ AWS S3 Security Pt.1

ğŸ“ Some types of S3 Security...

1. User-based. 
    - In this case, we can create IAM Policies (with identity access, which API calls should be allowed for a specific user); **<< Important for the AWS ML**
2. Resource Based 
    - Bucket Policies - bucket wide rules from the S3 console - allow cross account; **<< Important for the AWS ML**
    - Object Access Control List (ACL) - finer grain; 
    - Bucket Access Control List (ACL) - less common;

But, from a ML perspective, User-based and Bucket Policies will be most likely used. So...

- S3 Bucket Policies
    - JSON Based policies
        - Resources: buckets and objects;
        - Actions: set of API to Allow or Deny;
        - Effect: allow / deny;
        - Principal: the account or usr to apply the policy to
    - Use S3 bucket for policy to
        - Grant public access to the bucket;
        - Force objects to be encrypted at upload;
        - Grant access to another account (Cross Account)

<br>

### ğŸ“Œ S3 Default Encryption VS Bucket Policies
The old way to enable Default Encryption was to use a bucket policy and refuse any HTTP command without the proper header, the new way is to use the "default encryption" option in S3 (JSON). Note: Bucket Policies are evaluated before "default encryption".

<br>

### ğŸ“Œ AWS S3 Security Pt.2
ğŸ“ Other tools for security in S3...
- Networking - VPC Endpoint Gateway:
    - Allow traffic to stay within my VPC (instead of going through public web);
    - Make sure your privte services (AWS SageMaker) can access S3
    - **Very important for AWS ML**
- Logging and Audit:
    - S3 access logs can be stored in other S3 bucket;
    - API calls can be logged in AWS CloudTrail (consult who did and why);
- Tagged Based (IAM Policies + Bucket Policies)
    - Example: Add tag Classification=PHI to your objects;

<br>


## 2. Kinesis Data Stream & Kinesis Data Firehose 

### ğŸ“Œ AWS Kinesis Overview
#### â­ Extremely Important Subject for AWS ML â­<br>
Kinesis is a managed alternative to Apache Kafka. It is great for applications logs, metrics, IoT, and clickstreams; also great for "real-time" big data and streaming processing frameworks (Spark, NiFi, etc...). In Kinesis, all the data is automatically replicated synchronously to 3 AZ.

ğŸ“ Kinesis products...
- Kinesis Streams: low latency straming ingest at scale;
- Kinesis Analytics: perform real-time analytics on streams using SQL;
- Kinesis Firehose: load strams into S3, Redshift, ElasticSearch & Splunk;
- Kinesis Video Streams: meant for streaming video in real-time;

From a archtecture point of view Kinesis looks like...
1. Click streams, IoT devices, Metrics & Logs will be onboard by "Amazon Kinesis Streams";
2. Then we perfom some real time analytics with them, using "Amazon Kinesis Analytics";
3. Then, finally, we move those analytics to "Amazon Kinesis Firehose" that will put all data in Amazon S3 bucket or Amazon Redshift;

```
Obs.: "Amazon Kinesis Streams", "Amazon Kinesis Analytics" and "Amazon Kinesis Firehose" are "Amazon Kinesis"
```

### ğŸ“Œ Kinesis Data Streams 
ğŸ“ Overview...
- Streams are divided in ordered Shards/Partitions.
- Shards have to be provisioned in advance (capacity panning).
- More Shards = More capacity and speed.
    <br>-- Producers --> | Shard 1 / Shard 2 / Shard 3 | -- Consumers -->
- Data retention is 24 hours by default, can go up to 365 days.
- Ability to reprocess/replay data.
- Multiple applications can consume the same stream.
- Once data is inserted in Kinesis, it can't be deleted (immutability).
- Records can be up to 1MB in size.

ğŸ“ Limits to know...
- Producer
    - 1MB/s or 1000 messages/s at write PER SHARD
    - "ProvisionedThroughputException" otherwise
- Consumer Classic
    - 2 MB/s at read PER SHARD across all consumers
    - 5 API calls per second PER SHARD across all consumers
- Data Retention
    - 24 hours data retention by default
    - Can be extended to 365 days

Kinesis Stream is great fo real time streaming applications. It is something that you will need provision in advance.

### ğŸ“Œ Kinesis Data Firehose
Kinesis Data Firehose is to store data in different places.
<br><br>We have producers (Applications, Client, SDK, KPL, Kinesis Agent) <br>--> The data they produce will be read by Kinesis Data Streams (most common), or Amazon CloudWatch (Logs & Events), or AWS IoT <br>--> And then finally processed bt Kinesis Data Firehose, recorded one by one, up to 1MB each one. And if you want to transform de record, we have Lambda Function for Data Transformation in this step <br>--> After that is created a Batch Writes of all the information and move it to the target destination <br>--> The destinations can be...
- 3rd-party Partner Destinations
    - Datadog
    - Splunk
    - New Relic
    - MongoDB
- AWS Destinations
    - Amazon S3
    - Amazon Redshift (COPY through S3)
    - Amazon ElasticSearch
- Custom Destinations
    - HTTP Endpoint

ğŸ“ Overview...
- Fully Managed Service, no administration
- Near Real Time (60 seconds latency minimum for non full batches)
- Data Integration into Redshift / Amazon S3 / ElasticSearch / Splunk
- Automatic Scaling
- Supports many data formats
- Data Conversions from CSV / JSON to Parquet / ORC (only for S3)
- Data transformation through AWS Lambda (ex.: CSV => JSON)
- Supports compression when target is Amazon S3 (GZIP, ZIP, and SNAPPY)
- Pay for the amount of data going through Firehose 

### ğŸ“Œ Kinesis Data Streams vs Firehose
- Streams
    - Going to write custom code (producer / consumer) - real time applications
    - Real time (~200ms latency for classic, ~70ms latency for enhaced fan-out)
    - Must manage scaling (shard splitting (more shard) / merging (less shard))
    - Data Storage for 1 to 365 days, replay capability and multi consumers
- Firehose
    - Delivery service, **ingestion** service.
    - Fully managed, send to S3, Splunk, Redshift, ElasticSearch
    - Serverless data transformations with Lambda
    - **Near** real time (lowest buffer time is 1 minute)
    - Automated Scaling
    - No data storage

### ğŸ“Œ Kinesis Analytics, Conceptually...

Amazon Kinesis Data Streams + Amazon Kinesis Dara Firehose --> Amazon Kinesis Data Analytics --> Analytics Tools, Outputs


### ğŸ“Œ Kinesis Analytics, In more depth...
Input Stream(s) (Amazon Kinesis Data Streams, Amazon Kinesis Data Firehose) + Reference Table (Amazon S3) --> Amazon Kinesis Data Analytics ("SELECT STREAM (ItemID, count(*) FROM SourceStream GROUP BY ItemID)") --> Output Streams (Amazon Kinesis Data Streams + Amazon Kinesis Data Firehose --> Amazon S3 + Amazon Redshift) or Error Stream (query goes wrong)

### ğŸ“Œ Kinesis Data Analytics..
- Use cases
    - Streaming ETL: select columns, make simple transformations, on streaming data (ex.: reduce size of data set by selecting columns, making that with simple transformations in streaming data)
    - Continuous metric generation: live leaderboard for a mobile game
    - Responsive analytics: look for certain criteria and build alerting (filtering)
- Features
    - Pay only for resources consumed (but its not cheap)
    - Serverless: scales automatically
    - Use IAM permissions to access streaming source and destinations
    - SQL for Flink to write the computation
    - Schema discovery
    - Lambda can be used for pre-processing
    
### ğŸ“Œ Machine Learning on Kinesis Data Analytics..
- Algorithms to remember...
    - RANDOM_CUT_FOREST
        - SQL function used for anomaly detection on numeric columns in a stream
        - Example: detect anomalous subway ridership during the NYC marathon
        - Uses recent history to compute model 
    - HOTSPOTS
        - locate and return information about relatively dense regions in your data
        - Example: a collection of overheated servers in a data
        
### ğŸ“Œ Kinesis Video Stream
- Producers
    - Security camera, body-worn camera, AWS DeepLens, smartphone camera, audio feeds, images, RADAR data, RTSP camera.
    Producer SDK + DeepLens ---> Kinesis Video Stream
    - One producesr per video stream
- Video playback capability
- Consumers
    - build your own (MXNet, Tensorflow)
    - AWS SageMaker
    - Amazon Rekognition Video
- Keep data for 1 hour to 10 years - data retention


### ğŸ“Œ Kinesis Video Stream Use Cases
Kinesis Video Stream -- 1. Consume video stream in real-time -->
Fargate (Docker) -- 2. Checkpoint Stream Processing status (Consumer) --> 3. Send decoded frames for ML-based inference
<br> --> Fargate (Docker) -- 4. Publish inference results --> Kinesis Data Streams -- 5. Notifications --> AWS Lambda


### ğŸ“Œ Kinesis Summary -- Machine Learning
- Kinesis Data Stream: create real-time machine learning applications
- Kinesis Data Firehose: ingest massive data near-real time
- Kinesis Data Analytics: real-time ETL/ML algorithms on streams
- Kinesis Video Stream: real-time video stream to create ML applications

<br>

## 2. Glue Data Catalog & Crawlers
- Metadata repository for all your tables
    - Automated Schema Inference
    - Schemas are versioned
    Amazon S3 -- index --> AWS Glue Data Catalog
- Integrate with Athena or Redshift Spectrum (schema & data discovery)
    Amazon S3 -- index --> AWS Glue Data Catalog -- use schema --> AmazonRedshift/Amazon Athena --> Amazon QuicjSight/Amazon EMR
- The Glue Crawlers can help build the Glue Data Catalog

### ğŸ“Œ Glue Data Catalog - Crawlers
- Creawlers go through your data to infer schemas and partitions 
- Works JSON, Parquet, CSV, relational store
- Crawlers work for: S3, Amazon Redshift, Amazon RDS
- Run the Crawler ona Schedule or On Demand
- Need an IAM role/credentials to access the data stores

### ğŸ“Œ Glue and S3 Partitions
- Glue crawler will extract partitions based on how your S3 data is organized
- Think up front about how you will be querying your data lake in S3
- Exemple: devices send sensor data every hour
    - Do you query primarily by time ranges?
        - If so, organize your buckets as s3://my-bucket/dataset/yyyy/mm/dd/device
    - Do you query primarily by device?
        - If so, organize your buckets as s3://my-bucket/dataset/device/yyyy/mm/dd

## 3. Glue ETL
- Transform Data, Clean Data, Enrich Data (before doing analysis)
    - Generate ETL code in Python or Scala, you can modify the code
    - Can provide your own Spark or PySpark scripts
    - Target can be S3, JDBC (RDS, Redshift), or in Glue Data Catalog
- Fully managed, cost effective, pay only for the resources consumed
- Jobs are run on a serverless Spark platform 
- Glue Scheduler to schedule the jobs
- Glue Triggers to automate job runs based on "Events"

### ğŸ“Œ Glue ETL - Transformations
- Bundled Transformations
    - DropFields, DropNullFields - remove (null) fields
    - Filter - specify a function to filter records
    - Join - to enrich data
    - Map - add fields, delete fields, perform external lookups
- Machine Learning Transformations
    - FindMatches ML: identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly
- Format conversions: CSV, JSON, Avro, Parquet, ORC, XML 
- Apache Spark transformations (example: K-Means)

<br>

## 4. AWS Data Stores for Machine Learning
- Redshift
    - Data Warehousing, SQL Analytics (OLAP - Online Analytical Processing) 
    - Load data from S3 to Redshift
    - Use Redshift Spectrum to query data directly in S3 (no loading)
- RDS, Aurora
    - Relational Store, SQL (OLTP, Online Transaction Processing)
    - Must provision servers in advance
- Dynamo DB 
    - NoSQL data store, serverless, provision read/write capacity
    - Useful to store a machine learning model served by your application
- S3
    - Object storage
    - Serverless, infinite storage
    - Easy Integration 
- ElasicSearch
    - Indexing of data
    - Search amongst data ponits
    - Clickstram analytics
- ElastiCache
    - Caching mechanism
    - Not really used for Machine Learning
    